/**
 * Example 8: Query Explainability (Ollama + LM Studio)
 * 
 * This example demonstrates the Query Explainability feature that shows
 * WHY each document was retrieved. Tests both providers in one file:
 * - Ollama: Local AI platform
 * - LM Studio: User-friendly local AI interface
 * 
 * Benefits:
 * - Debugging RAG results
 * - Understanding search behavior
 * - Improving query quality
 * - Validating retrieval accuracy
 */

import {
  OllamaRAGClient,
  createOllamaRAGEmbedding,
  LMStudioRAGClient,
  createLMStudioRAGEmbedding,
  InMemoryVectorStore,
  Retriever,
  generateWithRAG
} from '../src/index.js';

async function testOllama() {
  console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
  console.log('ğŸ¦™ OLLAMA TEST');
  console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n');
  
  // Initialize RAG with Ollama
  console.log('âš™ï¸  Setting up Ollama RAG system...');
  const client = new OllamaRAGClient();
  const embed = createOllamaRAGEmbedding(client, 'nomic-embed-text');
  const store = new InMemoryVectorStore(embed);
  const retriever = new Retriever(store, { k: 3 });

  // Add sample documents
  const docs = [
    {
      text: 'Ollama is a local AI platform that runs large language models on your computer. It supports Llama, Mistral, and many other models.',
      meta: { source: 'documentation', topic: 'ollama' }
    },
    {
      text: 'LM Studio is another local AI solution that provides a user-friendly interface for running language models. It includes model management and API server.',
      meta: { source: 'documentation', topic: 'lmstudio' }
    },
    {
      text: 'Python is a popular programming language widely used for AI and machine learning. It has libraries like TensorFlow and PyTorch.',
      meta: { source: 'tutorial', topic: 'programming' }
    },
    {
      text: 'JavaScript is essential for web development. Modern frameworks like React, Vue, and Angular make building web apps easier.',
      meta: { source: 'tutorial', topic: 'programming' }
    }
  ];

  console.log('ğŸ“š Adding documents...');
  for (const doc of docs) {
    await store.addDocuments([doc]);
  }
  console.log(`âœ… Added ${docs.length} documents\n`);

  // Test with explanation
  const query = 'local AI models on computer';
  console.log(`ğŸ” Query: "${query}"\n`);

  const results = await retriever.getRelevant(query, 2, {
    explain: true
  });

  results.forEach((result, index) => {
    console.log(`Result ${index + 1}:`);
    console.log(`Text: ${result.text.substring(0, 80)}...`);
    console.log(`\nğŸ“Š Explanation:`);
    console.log(`  Query Terms: [${result.explanation.queryTerms.join(', ')}]`);
    console.log(`  Matched Terms: [${result.explanation.matchedTerms.join(', ')}]`);
    console.log(`  Match Count: ${result.explanation.matchCount}/${result.explanation.queryTerms.length}`);
    console.log(`  Coverage: ${result.explanation.relevanceFactors.coverage}`);
    console.log(`  Cosine Similarity: ${result.explanation.cosineSimilarity}\n`);
  });

  // Generate answer with LLM
  console.log('ğŸ¤– Generating answer with Ollama LLM (granite4:3b)...\n');
  const userQuestion = 'What is Ollama and how does it work?';
  const relevantDocs = await retriever.getRelevant(userQuestion, 2, { explain: true });
  
  const answer = await generateWithRAG(
    client,
    'granite4:3b',
    userQuestion,
    relevantDocs.map(doc => doc.text),
    {
      systemPrompt: 'You are a helpful AI assistant. Answer concisely based on the provided context.'
    }
  );

  console.log('âœ¨ Generated Answer:');
  console.log(answer);
  console.log(`\nğŸ“Š Answer Quality:
   â€¢ Retrieved: ${relevantDocs.length} documents
   â€¢ Avg Coverage: ${Math.round(relevantDocs.reduce((sum, doc) => sum + doc.explanation.matchRatio, 0) / relevantDocs.length * 100)}%
   â€¢ Matched Terms: [${relevantDocs[0].explanation.matchedTerms.join(', ')}]
   â€¢ LLM: granite4:3b\n`);
}

async function testLMStudio() {
  console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
  console.log('ğŸ¨ LM STUDIO TEST');
  console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n');
  
  // Initialize RAG with LM Studio
  console.log('âš™ï¸  Setting up LM Studio RAG system...');
  const client = new LMStudioRAGClient();
  const embed = createLMStudioRAGEmbedding(client, 'nomic-embed-text-v1.5');
  const store = new InMemoryVectorStore(embed);
  const retriever = new Retriever(store, { k: 3 });

  // Add same documents
  const docs = [
    {
      text: 'Ollama is a local AI platform that runs large language models on your computer. It supports Llama, Mistral, and many other models.',
      meta: { source: 'documentation', topic: 'ollama' }
    },
    {
      text: 'LM Studio is another local AI solution that provides a user-friendly interface for running language models. It includes model management and API server.',
      meta: { source: 'documentation', topic: 'lmstudio' }
    },
    {
      text: 'Python is a popular programming language widely used for AI and machine learning. It has libraries like TensorFlow and PyTorch.',
      meta: { source: 'tutorial', topic: 'programming' }
    },
    {
      text: 'JavaScript is essential for web development. Modern frameworks like React, Vue, and Angular make building web apps easier.',
      meta: { source: 'tutorial', topic: 'programming' }
    }
  ];

  console.log('ğŸ“š Adding documents...');
  for (const doc of docs) {
    await store.addDocuments([doc]);
  }
  console.log(`âœ… Added ${docs.length} documents\n`);

  // Test with explanation
  const query = 'local AI models on computer';
  console.log(`ğŸ” Query: "${query}"\n`);

  const results = await retriever.getRelevant(query, 2, {
    explain: true
  });

  results.forEach((result, index) => {
    console.log(`Result ${index + 1}:`);
    console.log(`Text: ${result.text.substring(0, 80)}...`);
    console.log(`\nğŸ“Š Explanation:`);
    console.log(`  Query Terms: [${result.explanation.queryTerms.join(', ')}]`);
    console.log(`  Matched Terms: [${result.explanation.matchedTerms.join(', ')}]`);
    console.log(`  Match Count: ${result.explanation.matchCount}/${result.explanation.queryTerms.length}`);
    console.log(`  Coverage: ${result.explanation.relevanceFactors.coverage}`);
    console.log(`  Cosine Similarity: ${result.explanation.cosineSimilarity}\n`);
  });

  // Generate answer with LLM
  console.log('ğŸ¤– Generating answer with LM Studio LLM...\n');
  const userQuestion = 'What is LM Studio and what features does it offer?';
  const relevantDocs = await retriever.getRelevant(userQuestion, 2, { explain: true });
  
  const answer = await generateWithRAG(
    client,
    'qwen3-vl-4b-instruct', // LM Studio model (from available models list)
    userQuestion,
    relevantDocs.map(doc => doc.text),
    {
      systemPrompt: 'You are a helpful AI assistant. Answer concisely based on the provided context.'
    }
  );

  console.log('âœ¨ Generated Answer:');
  console.log(answer);
  console.log(`\nğŸ“Š Answer Quality:
   â€¢ Retrieved: ${relevantDocs.length} documents
   â€¢ Avg Coverage: ${Math.round(relevantDocs.reduce((sum, doc) => sum + doc.explanation.matchRatio, 0) / relevantDocs.length * 100)}%
   â€¢ Matched Terms: [${relevantDocs[0].explanation.matchedTerms.join(', ')}]
   â€¢ LLM: LM Studio (qwen3-vl-4b-instruct)\n`);
}

async function main() {
  console.log('ğŸ” Query Explainability - Dual Provider Test\n');

  console.log('Prerequisites:');
  console.log('1. Ollama: ollama serve & ollama pull nomic-embed-text & ollama pull granite4:3b');
  console.log('2. LM Studio: Start server & load a model & load nomic-embed-text-v1.5\n');
  console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n');

  // Test Ollama
  try {
    await testOllama();
  } catch (error) {
    console.error('âŒ Ollama test failed:', error.message);
    console.log('Make sure Ollama is running: ollama serve\n');
  }

  console.log('\n');

  // Test LM Studio
  try {
    await testLMStudio();
  } catch (error) {
    console.error('âŒ LM Studio test failed:', error.message);
    console.log('Make sure LM Studio server is running on http://localhost:1234\n');
  }

  // Summary
  console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
  console.log('ğŸ“‹ COMPARISON SUMMARY');
  console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n');
  console.log('âœ¨ Query Explainability works with both providers!');
  console.log('\nğŸ” Key Features Tested:');
  console.log('  âœ… Query term extraction');
  console.log('  âœ… Term matching detection');
  console.log('  âœ… Match coverage calculation');
  console.log('  âœ… Cosine similarity scoring');
  console.log('  âœ… Relevance factor breakdown');
  console.log('  âœ… RAG generation with explanations');
  console.log('\nğŸ’¡ Use Cases:');
  console.log('  â€¢ Debug unexpected search results');
  console.log('  â€¢ Understand semantic vs. keyword matching');
  console.log('  â€¢ Optimize query quality');
  console.log('  â€¢ Validate retrieval accuracy');
  console.log('  â€¢ Explain results to end users');
  console.log('\nğŸ¯ Unique Feature: No other RAG library offers this level of explainability!\n');
}

main().catch(console.error);
