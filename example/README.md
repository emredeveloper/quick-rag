# Quick RAG Examples âš¡# Example Demos - RAG Local LLM v0.6.3



Simple, focused examples to get you started quickly.This folder contains examples demonstrating all features.



## ğŸš€ Quick Start## ğŸš€ Quick Start



### 1ï¸âƒ£ Ollama Example (Recommended)### Simple Example (Recommended)



Official Ollama SDK with streaming support:```bash

node example/simple-nodejs.js

```bash```

node example/official-ollama-example.js

```**Output:**

```

**Features:**ğŸ“š Retrieved Documents:

- âœ… Official SDK integration

- âœ… Real-time streaming1. JavaScript is a programming language.

- âœ… Vector search with RAG   ID: 1 | Relevance: 80.2%

- âœ… Model management

- âœ… Clean, readable output2. Python is great for data science.

   ID: 2 | Relevance: 38.8%

### 2ï¸âƒ£ LM Studio Example

ğŸ¤– AI Answer:

Test multiple models at once:JavaScript is a programming language...

```

```bash

node example/official-lmstudio-example.js### All Examples

```

All examples work with a running Ollama server. Make sure you have:

**Features:**```bash

- âœ… Tests all downloaded modelsollama pull granite4:tiny-h

- âœ… Official LM Studio SDKollama pull embeddinggemma

- âœ… Automatic model loadingollama serve

- âœ… Comparison output```



### 3ï¸âƒ£ Simple Node.js## ğŸ“ Examples



Basic example without streaming:### ğŸŒŸ **simple-nodejs.js** - Clean & Simple (NEW!)

Perfect starting point with clean output:

```bash- âœ… Minimal setup

node example/simple-nodejs.js- âœ… Clean console output

```- âœ… Easy to understand



**Features:**```bash

- âœ… Minimal setupnode example/simple-nodejs.js

- âœ… Easy to understand```

- âœ… Perfect for beginners

### ğŸ”§ **pure-nodejs-example.js** - Detailed Output

---Same as simple but with more details:

- ğŸ“Š Step-by-step process

## ğŸ“‹ Prerequisites- ğŸ” Detailed logging



### For Ollama Examples```bash

node example/pure-nodejs-example.js

```bash```

# Install Ollama

curl -fsSL https://ollama.com/install.sh | sh### 1. **all-features-demo.js** - Complete Feature Showcase

Demonstrates ALL new features in one comprehensive demo:

# Pull models- âš¡ Batch embedding with Promise.all

ollama pull granite4:tiny-h- ğŸ“š CRUD operations (add, update, delete, get)

ollama pull embeddinggemma- ğŸ¯ Dynamic topK parameter

- ğŸŒŠ Prompt return for streaming

# Start server- ğŸš€ Modern fetch support

ollama serve

``````bash

node example/all-features-demo.js

### For LM Studio Examples```



1. Download [LM Studio](https://lmstudio.ai/)### 2. **topk-example.js** - Dynamic topK Parameter

2. Download models from the UIShows how the `topK` parameter now works correctly:

3. Start local server: `Developer > Local Server`- Default retriever behavior (k=2)

4. Server runs at `http://localhost:1234`- Override with different topK values (3, 5, 10)

- Integration with generateWithRAG

---

```bash

## ğŸ“‚ Advanced Examplesnode example/topk-example.js

```

More complex examples are in `example/advanced/`:

### 3. **crud-example.js** - VectorStore CRUD Operations

- `all-features-demo.js` - Complete feature showcaseDemonstrates the new document management methods:

- `batch-embedding-example.js` - Batch processing- `getAllDocuments()` - Get all documents

- `crud-example.js` - CRUD operations- `getDocument(id)` - Get specific document

- `streaming-example.js` - Streaming responses- `updateDocument(id, text, meta)` - Update and re-embed

- `topk-example.js` - Dynamic retrieval- `deleteDocument(id)` - Remove document

- `mrl-example.js` - Matryoshka embeddings- `clear()` - Clear all documents



---```bash

node example/crud-example.js

## ğŸ†˜ Troubleshooting```



| Problem | Solution |### 4. **batch-embedding-example.js** - Performance Improvement

|---------|----------|Shows the massive performance gain from parallel embedding:

| `Connection refused` | Start Ollama: `ollama serve` or LM Studio server |- Sequential vs parallel embedding comparison

| `Model not found` | Pull model: `ollama pull <model>` or download in LM Studio |- Real-world performance metrics

| `Import errors` | Run from project root: `node example/...` |- Works with 80+ documents



---```bash

node example/batch-embedding-example.js

## ğŸ“– Learn More```



- [Main README](../README.md) - Full documentation### 5. **streaming-example.js** - Streaming Support

- [Official Ollama SDK](https://github.com/ollama/ollama-js)Demonstrates how generateWithRAG now returns prompts:

- [Official LM Studio SDK](https://github.com/lmstudio-ai/lmstudio-js)- Prompt structure and generation

- Streaming integration (like useRAG hook)

**Made with âš¡ by Quick RAG**- Backward compatibility


```bash
node example/streaming-example.js
```

### 6. **run.js** (Original) - Basic RAG Example
The original simple RAG orchestration example.

```bash
node example/run.js
```

### 7. **mrl-example.js** (Original) - MRL Embedding
Demonstrates Matryoshka Representation Learning with different dimensions.

```bash
node example/mrl-example.js
```

## ğŸ”§ Environment Variables

- `OLLAMA_TEST=1` - Enable real Ollama API calls
- `OLLAMA_MODEL=<model>` - Set model (default: granite4:tiny-h)

## ğŸ“Š Expected Output

Each example includes:
- âœ… Success indicators
- ğŸ“Š Performance metrics
- ğŸ¯ Feature demonstrations
- âŒ Error handling

## ğŸ¯ What's New in v0.6.0?

### Critical Fixes
- âœ… Removed circular dependency
- âœ… Fixed topK parameter handling
- âœ… Fixed streaming support

### New Features
- âš¡ 100x faster batch embedding
- ğŸ“š Full CRUD operations
- ğŸ¯ Dynamic topK parameter
- ğŸŒŠ Streaming-ready prompt return
- ğŸš€ Modern fetch (Node 18+)

## ğŸ“ Notes

- All examples run without Ollama (mock mode) by default
- Set `OLLAMA_TEST=1` for real API calls
- Make sure Ollama is running on `localhost:11434` if testing with it
- Examples use dimension 128 for speed (MRL allows this!)

## ğŸ¤ Contributing

See more examples? Have suggestions? Open an issue or PR!
