/**
 * Quick RAG - LM Studio Streaming
 * 
 * Shows how to stream responses from LM Studio for better UX
 * 
 * Prerequisites:
 * - LM Studio running with local server enabled (http://localhost:1234)
 * - A model loaded in LM Studio
 */

import { 
  LMStudioRAGClient,
  createLMStudioRAGEmbedding,
  InMemoryVectorStore, 
  Retriever,
  generateWithRAG
} from 'quick-rag';

async function main() {
  console.log('ğŸš€ Quick RAG - LM Studio Streaming\n');

  try {
    // Initialize
    const client = new LMStudioRAGClient();
    const embed = createLMStudioRAGEmbedding(client, 'text-embedding-nomic-embed-text-v1.5');
    const vectorStore = new InMemoryVectorStore(embed);
    const retriever = new Retriever(vectorStore);

    // Add knowledge base
    console.log('ğŸ“š Building knowledge base...\n');
    
    await vectorStore.addDocuments([
      {
        text: "Artificial Intelligence (AI) is the simulation of human intelligence by machines. Modern AI includes machine learning, where systems learn from data, and deep learning, which uses neural networks. AI applications range from voice assistants to autonomous vehicles.",
        meta: { category: "AI" }
      },
      {
        text: "Large Language Models (LLMs) are AI systems trained on vast amounts of text data. They can understand and generate human-like text. Examples include GPT-4, Claude, and Llama. LLMs are used for chatbots, content generation, and code assistance.",
        meta: { category: "AI" }
      },
      {
        text: "Retrieval Augmented Generation (RAG) combines language models with information retrieval. Instead of relying only on training data, RAG systems can access external documents in real-time. This reduces hallucinations and keeps responses up-to-date.",
        meta: { category: "AI" }
      }
    ]);

    console.log('âœ… Knowledge base ready\n');

    // Example 1: Non-streaming (for comparison)
    console.log('â•'.repeat(70));
    console.log('ğŸ” Example 1: Regular Response (Non-Streaming)');
    console.log('â•'.repeat(70) + '\n');

    const query1 = 'What is RAG and why is it useful?';
    console.log(`â“ Question: ${query1}\n`);

    const docs1 = await retriever.getRelevant(query1, 2);
    console.log(`ğŸ“š Found ${docs1.length} relevant document(s)\n`);

    console.log('â³ Waiting for complete response...\n');
    const startTime1 = Date.now();

    const response1 = await generateWithRAG(
      client,
      'qwen/qwen3-4b-2507',
      query1,
      docs1.map(d => d.text),
      {
        systemPrompt: 'You are an AI expert. Explain concepts clearly and concisely.'
      }
    );

    const elapsed1 = Date.now() - startTime1;
    console.log('ğŸ¤– Complete Answer:');
    console.log('-'.repeat(70));
    console.log(response1.response);
    console.log('-'.repeat(70));
    console.log(`â±ï¸  Time: ${(elapsed1 / 1000).toFixed(2)}s\n`);

    // Example 2: Streaming response
    console.log('â•'.repeat(70));
    console.log('ğŸ” Example 2: Streaming Response (Better UX)');
    console.log('â•'.repeat(70) + '\n');

    const query2 = 'Explain how Large Language Models work';
    console.log(`â“ Question: ${query2}\n`);

    const docs2 = await retriever.getRelevant(query2, 2);
    console.log(`ğŸ“š Found ${docs2.length} relevant document(s)\n`);

    console.log('ğŸŒŠ Streaming response:\n');
    console.log('-'.repeat(70));

    const startTime2 = Date.now();

    // Use LM Studio client's streaming capability
    const stream = await client.chat({
      model: 'qwen/qwen3-vl-4b',
      messages: [
        {
          role: 'system',
          content: 'You are an AI expert. Explain concepts clearly using the provided context.'
        },
        {
          role: 'user',
          content: `Context:\n${docs2.map(d => d.text).join('\n\n')}\n\nQuestion: ${query2}`
        }
      ],
      stream: true,
      temperature: 0.7,
      max_tokens: 500
    });

    let fullResponse = '';
    
    for await (const chunk of stream) {
      const content = chunk.choices[0]?.delta?.content || '';
      if (content) {
        process.stdout.write(content);
        fullResponse += content;
      }
    }

    const elapsed2 = Date.now() - startTime2;
    console.log('\n' + '-'.repeat(70));
    console.log(`â±ï¸  Time: ${(elapsed2 / 1000).toFixed(2)}s\n`);

    // Example 3: Streaming with progress indicators
    console.log('â•'.repeat(70));
    console.log('ğŸ” Example 3: Streaming with Token Count');
    console.log('â•'.repeat(70) + '\n');

    const query3 = 'What are the main applications of AI?';
    console.log(`â“ Question: ${query3}\n`);

    const docs3 = await retriever.getRelevant(query3, 2);

    console.log('ğŸŒŠ Streaming with metrics:\n');
    console.log('-'.repeat(70) + '\n');

    const startTime3 = Date.now();

    const stream3 = await client.chat({
      model: 'google/gemma-3-4b',
      messages: [
        {
          role: 'system',
          content: 'You are an AI expert. Provide practical examples and applications.'
        },
        {
          role: 'user',
          content: `Context:\n${docs3.map(d => d.text).join('\n\n')}\n\nQuestion: ${query3}`
        }
      ],
      stream: true,
      temperature: 0.7,
      max_tokens: 400
    });

    let tokenCount = 0;
    let responseText = '';

    for await (const chunk of stream3) {
      const content = chunk.choices[0]?.delta?.content || '';
      if (content) {
        process.stdout.write(content);
        responseText += content;
        tokenCount++;
      }
    }

    const elapsed3 = Date.now() - startTime3;
    const tokensPerSecond = (tokenCount / (elapsed3 / 1000)).toFixed(1);

    console.log('\n\n' + '-'.repeat(70));
    console.log(`ğŸ“Š Metrics:`);
    console.log(`   â€¢ Tokens: ${tokenCount}`);
    console.log(`   â€¢ Time: ${(elapsed3 / 1000).toFixed(2)}s`);
    console.log(`   â€¢ Speed: ${tokensPerSecond} tokens/second\n`);

    console.log('âœ… LM Studio streaming completed!\n');

    console.log('ğŸ’¡ Streaming Benefits:');
    console.log('   â€¢ Immediate feedback - users see response forming');
    console.log('   â€¢ Better perceived performance');
    console.log('   â€¢ Can cancel long responses early');
    console.log('   â€¢ More engaging user experience\n');

    console.log('ğŸ”§ Streaming vs Non-Streaming:');
    console.log('   â€¢ Non-streaming: Wait for full response, then display');
    console.log('   â€¢ Streaming: Display tokens as they arrive');
    console.log('   â€¢ Both methods produce the same final result\n');

  } catch (error) {
    if (error.message.includes('ECONNREFUSED') || error.message.includes('fetch failed')) {
      console.error('\nâŒ Error: Cannot connect to LM Studio');
      console.error('\nğŸ“‹ Make sure:');
      console.error('   1. LM Studio is running');
      console.error('   2. Local server is started');
      console.error('   3. A model is loaded\n');
    } else {
      console.error('âŒ Error:', error.message);
      console.error(error.stack);
    }
    process.exit(1);
  }
}

main().catch(error => {
  console.error('âŒ Error:', error.message);
  process.exit(1);
});
